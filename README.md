# ğŸš€ Awesome Model Compression Techniques

A curated collection of hands-on examples demonstrating key **model compression techniques** in deep learning.  
Compress your neural networks without compromising too much on performance! ğŸ’¡

---

## ğŸ“š Whatâ€™s Inside

| Technique | Description | Example |
|----------|-------------|---------|
| ğŸ§¹ **Pruning** | Remove unimportant weights or neurons to reduce model size and speed up inference | [Go to notebook](./pruning/) |
| âš–ï¸ **Quantization** | Convert weights from 32-bit floats to 8-bit or even binary to save memory | [Go to notebook](./quantization/) |
| ğŸ§  **Knowledge Distillation** | Train a small model (student) to mimic a large one (teacher) | [Go to notebook](./distillation/) |
| ğŸ§¬ **Neural Architecture Search (NAS)** | Automatically discover optimal architectures under constraints | [Go to notebook](./nas/) |
| ğŸ”» **Low-Rank Factorization** | Decompose large weight matrices into smaller ones | [Go to notebook](./lowrank/) |
| â© **Dynamic Inference** | Adapt computation dynamically based on input | [Go to notebook](./dynamic_inference/) |

---

## ğŸ› ï¸ How to Use

Clone the repository and start exploring each technique through Jupyter notebooks:

```bash
git clone https://github.com/your-username/model-compression-lab.git
cd model-compression-lab
