# 🚀 Awesome Model Compression Techniques

A curated collection of hands-on examples demonstrating key **model compression techniques** in deep learning.  
Compress your neural networks without compromising too much on performance! 💡

---

## 📚 What’s Inside

| Technique | Description | Example |
|----------|-------------|---------|
| 🧹 **Pruning** | Remove unimportant weights or neurons to reduce model size and speed up inference | [Go to notebook](./pruning/) |
| ⚖️ **Quantization** | Convert weights from 32-bit floats to 8-bit or even binary to save memory | [Go to notebook](./quantization/) |
| 🧠 **Knowledge Distillation** | Train a small model (student) to mimic a large one (teacher) | [Go to notebook](./distillation/) |
| 🧬 **Neural Architecture Search (NAS)** | Automatically discover optimal architectures under constraints | [Go to notebook](./nas/) |
| 🔻 **Low-Rank Factorization** | Decompose large weight matrices into smaller ones | [Go to notebook](./lowrank/) |
| ⏩ **Dynamic Inference** | Adapt computation dynamically based on input | [Go to notebook](./dynamic_inference/) |

---

## 🛠️ How to Use

Clone the repository and start exploring each technique through Jupyter notebooks:

```bash
git clone https://github.com/your-username/model-compression-lab.git
cd model-compression-lab
